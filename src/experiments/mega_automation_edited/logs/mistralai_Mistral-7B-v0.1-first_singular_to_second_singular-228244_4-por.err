++ date
+ echo '[Wed Nov  5 11:34:57 AM CET 2025] Task 4 -> por'
+ echo 'Pair: first singular -> second singular'
+ echo 'Model: mistralai/Mistral-7B-v0.1'
+ srun python3 -u mega_automation.py --model_name mistralai/Mistral-7B-v0.1 --lang_iso3 por --person_a 'first singular' --person_b 'second singular'
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:15<00:15, 15.71s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:22<00:00, 10.59s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:22<00:00, 11.36s/it]
WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.98s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  1.98s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.13s/it]
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.40s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.62s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.74s/it]
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
  0%|          | 0/1024 [00:00<?, ?it/s]  0%|          | 0/1024 [00:01<?, ?it/s]
Traceback (most recent call last):
  File "/home/lis.isabella.gidi/jsalt2025/src/experiments/mega_automation_edited/mega_automation.py", line 183, in <module>
    activation_patching(
    ~~~~~~~~~~~~~~~~~~~^
        lang_iso3 = iso_code,
        ^^^^^^^^^^^^^^^^^^^^^
    ...<8 lines>...
        max_prompts_head  = args.max_prompts_head,
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/lis.isabella.gidi/jsalt2025/src/experiments/mega_automation_edited/mega_automation.py", line 127, in activation_patching
    run_attn_head_out_patching(
    ~~~~~~~~~~~~~~~~~~~~~~~~~~^
        tl_model, clean_txts, corrupt_txts, answers,
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
        direction_label=label, lang_tag=lang_name, device=device
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/lis.isabella.gidi/jsalt2025/src/experiments/mega_automation_edited/utils_mega_automation.py", line 664, in run_attn_head_out_patching
    patch = patching.get_act_patch_attn_head_out_all_pos(
        tl_model,
    ...<2 lines>...
        patching_metric=conjugation_metric,
    )
  File "/public/conda/user_envs/lis.isabella.gidi/envs/jsalt2025/lib/python3.13/site-packages/transformer_lens/patching.py", line 218, in generic_activation_patch
    patched_logits = model.run_with_hooks(
        corrupted_tokens, fwd_hooks=[(current_activation_name, current_hook)]
    )
  File "/public/conda/user_envs/lis.isabella.gidi/envs/jsalt2025/lib/python3.13/site-packages/transformer_lens/hook_points.py", line 447, in run_with_hooks
    return hooked_model.forward(*model_args, **model_kwargs)
           ~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/public/conda/user_envs/lis.isabella.gidi/envs/jsalt2025/lib/python3.13/site-packages/transformer_lens/HookedTransformer.py", line 620, in forward
    residual = block(
        residual,
    ...<4 lines>...
        attention_mask=attention_mask,
    )  # [batch, pos, d_model]
  File "/public/conda/user_envs/lis.isabella.gidi/envs/jsalt2025/lib/python3.13/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^
  File "/public/conda/user_envs/lis.isabella.gidi/envs/jsalt2025/lib/python3.13/site-pasrun: error: atos1: task 0: Exited with exit code 1
all_impl
    return forward_call(*args, **kwargs)
  File "/public/conda/user_envs/lis.isabella.gidi/envs/jsalt2025/lib/python3.13/site-packages/transformer_lens/components/transformer_block.py", line 160, in forward
    self.attn(
    ~~~~~~~~~^
        query_input=self.ln1(query_input)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    ...<5 lines>...
        attention_mask=attention_mask,
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/public/conda/user_envs/lis.isabella.gidi/envs/jsalt2025/lib/python3.13/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^
  File "/public/conda/user_envs/lis.isabella.gidi/envs/jsalt2025/lib/python3.13/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/public/conda/user_envs/lis.isabella.gidi/envs/jsalt2025/lib/python3.13/site-packages/transformer_lens/components/abstract_attention.py", line 302, in forward
    out = F.linear(
        z.reshape(z.shape[0], z.shape[1], self.cfg.d_head * self.cfg.n_heads),
        w,
        self.b_O,
    )
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 18.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 5.88 MiB is free. Including non-PyTorch memory, this process has 79.24 GiB memory in use. Of the allocated memory 78.42 GiB is allocated by PyTorch, and 336.28 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
