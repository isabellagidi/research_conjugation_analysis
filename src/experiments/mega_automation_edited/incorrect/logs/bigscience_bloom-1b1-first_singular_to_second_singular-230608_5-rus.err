++ date
+ echo '[Mon Nov 10 09:26:15 AM CET 2025] Task 5 -> rus'
+ echo 'Pair: first singular -> second singular'
+ echo 'Model: bigscience/bloom-1b1'
+ srun python3 -u mega_automation.py --model_name bigscience/bloom-1b1 --lang_iso3 rus --person_a 'first singular' --person_b 'second singular'
WARNING:root:Missing key for a weight matrix in pretrained, filled in with an empty tensor: pos_embed.W_pos
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
  0%|          | 0/384 [00:00<?, ?it/s]  0%|          | 1/384 [00:00<01:29,  4.27it/s]  0%|          | 1/384 [00:01<11:25,  1.79s/it]
Traceback (most recent call last):
  File "/home/lis.isabella.gidi/jsalt2025/src/experiments/mega_automation_edited/incorrect/mega_automation.py", line 183, in <module>
    activation_patching(
    ~~~~~~~~~~~~~~~~~~~^
        lang_iso3 = iso_code,
        ^^^^^^^^^^^^^^^^^^^^^
    ...<8 lines>...
        max_prompts_head  = args.max_prompts_head,
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/lis.isabella.gidi/jsalt2025/src/experiments/mega_automation_edited/incorrect/mega_automation.py", line 127, in activation_patching
    run_attn_head_out_patching(
    ~~~~~~~~~~~~~~~~~~~~~~~~~~^
        tl_model, clean_txts, corrupt_txts, answers,
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
        direction_label=label, lang_tag=lang_name, device=device
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/lis.isabella.gidi/jsalt2025/src/experiments/mega_automation_edited/incorrect/utils_mega_automation.py", line 664, in run_attn_head_out_patching
    patch = patching.get_act_patch_attn_head_out_all_pos(
        tl_model,
    ...<2 lines>...
        patching_metric=conjugation_metric,
    )
  File "/public/conda/user_envs/lis.isabella.gidi/envs/jsalt2025/lib/python3.13/site-packages/transformer_lens/patching.py", line 218, in generic_activation_patch
    patched_logits = model.run_with_hooks(
        corrupted_tokens, fwd_hooks=[(current_activation_name, current_hook)]
    )
  File "/public/conda/user_envs/lis.isabella.gidi/envs/jsalt2025/lib/python3.13/site-packages/transformer_lens/hook_points.py", line 447, in run_with_hooks
    return hooked_model.forward(*model_args, **model_kwargs)
           ~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/public/conda/user_envs/lis.isabella.gidi/envs/jsalt2025/lib/python3.13/site-packages/transformer_lens/HookedTransformer.py", line 620, in forward
    residual = block(
        residual,
    ...<4 lines>...
        attention_mask=attention_mask,
    )  # [batch, pos, d_model]
  File "/public/conda/user_envs/lis.isabella.gidi/envs/jsalt2025/lib/python3.13/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^
  File "/public/conda/user_envs/lis.isabella.gidi/envs/jsalt2025/lib/python3.13/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/public/conda/user_envs/lis.isabella.gidi/envs/jsalt2025/lib/python3.13/site-packages/transformer_lens/components/transformer_block.py", line 160, in forward
    self.attn(
    ~~~~~~~~~^
        query_input=self.ln1(query_input)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    ...<5 lines>...
        attention_mask=attention_mask,
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/public/conda/user_envs/lis.isabella.gidi/envs/jsalt2025/lib/python3.13/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^
  File "/public/conda/user_envs/lis.isabella.gidi/envs/jsalt2025/lib/python3.13/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/public/conda/user_envs/lis.isabella.gidi/envs/jsalt2025/lib/python3.13/site-packages/transformer_lens/components/abstract_attention.py", line 210, in forward
    q, k, v = self.calculate_qkv_matrices(query_input, key_input, value_input)
              ~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/public/conda/user_envs/lis.isabella.gidi/envs/jsalt2025/lib/python3.13/site-packages/transformer_lens/components/abstract_attention.py", line 414, in calculate_qkv_matrices
    k = self.hook_k(attn_fn(key_input, self.W_K, self.b_K))
                    ~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/public/conda/user_envs/lis.isabella.gidi/envs/jsalt2025/lib/python3.13/site-packages/transformer_lens/utilities/attention.py", line 27, in simple_attn_linear
    return F.linear(input, w, b_).reshape(input.shape[0], input.shape[1], b.shape[0], b.shape[1])
           ~~~~~~~~^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 4.75 MiB is free. Process 296917 has 13.40 GiB memory in use. Process 317397 has 13.40 GiB memory in use. Including non-PyTorch memory, this process has 52.42 GiB memory in use. Of the allocated memory 51.87 GiB is allocated by PyTorch, and 60.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
srun: error: selexini-1: task 0: Exited with exit code 1
