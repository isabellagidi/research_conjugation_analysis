Loaded pretrained model bigscience/bloom-1b1 into HookedTransformer
model: HookedTransformer(
  (embed): Embed(
    (ln): LayerNorm(
      (hook_scale): HookPoint()
      (hook_normalized): HookPoint()
    )
  )
  (hook_embed): HookPoint()
  (pos_embed): PosEmbed()
  (hook_pos_embed): HookPoint()
  (blocks): ModuleList(
    (0-23): 24 x TransformerBlock(
      (ln1): LayerNormPre(
        (hook_scale): HookPoint()
        (hook_normalized): HookPoint()
      )
      (ln2): LayerNormPre(
        (hook_scale): HookPoint()
        (hook_normalized): HookPoint()
      )
      (attn): Attention(
        (hook_k): HookPoint()
        (hook_q): HookPoint()
        (hook_v): HookPoint()
        (hook_z): HookPoint()
        (hook_attn_scores): HookPoint()
        (hook_pattern): HookPoint()
        (hook_result): HookPoint()
      )
      (mlp): MLP(
        (hook_pre): HookPoint()
        (hook_post): HookPoint()
      )
      (hook_attn_in): HookPoint()
      (hook_q_input): HookPoint()
      (hook_k_input): HookPoint()
      (hook_v_input): HookPoint()
      (hook_mlp_in): HookPoint()
      (hook_attn_out): HookPoint()
      (hook_mlp_out): HookPoint()
      (hook_resid_pre): HookPoint()
      (hook_resid_mid): HookPoint()
      (hook_resid_post): HookPoint()
    )
  )
  (ln_final): LayerNormPre(
    (hook_scale): HookPoint()
    (hook_normalized): HookPoint()
  )
  (unembed): Unembed()
)
Kept verb forms for language: deu

length of dataset (usually 500 but in this case): 100
✅ Saved: resid_pre_activation_patching_inf3_conj3_german.png
✅ Saved: resid_pre_activation_patching_inf2_conj3_german.png
✅ Saved: resid_pre_activation_patching_inf4_conj4_german.png
✅ Saved: resid_pre_activation_patching_inf2_conj2_german.png
✅ Saved: resid_pre_activation_patching_inf3_conj4_german.png
✅ Saved: attn_head_out_all_pos_first2second_german_second2first.png
