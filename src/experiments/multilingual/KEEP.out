
=== Running activation patching for catalan (cat) ===
Loaded pretrained model bigscience/bloom-1b1 into HookedTransformer
model: HookedTransformer(
  (embed): Embed(
    (ln): LayerNorm(
      (hook_scale): HookPoint()
      (hook_normalized): HookPoint()
    )
  )
  (hook_embed): HookPoint()
  (pos_embed): PosEmbed()
  (hook_pos_embed): HookPoint()
  (blocks): ModuleList(
    (0-23): 24 x TransformerBlock(
      (ln1): LayerNormPre(
        (hook_scale): HookPoint()
        (hook_normalized): HookPoint()
      )
      (ln2): LayerNormPre(
        (hook_scale): HookPoint()
        (hook_normalized): HookPoint()
      )
      (attn): Attention(
        (hook_k): HookPoint()
        (hook_q): HookPoint()
        (hook_v): HookPoint()
        (hook_z): HookPoint()
        (hook_attn_scores): HookPoint()
        (hook_pattern): HookPoint()
        (hook_result): HookPoint()
      )
      (mlp): MLP(
        (hook_pre): HookPoint()
        (hook_post): HookPoint()
      )
      (hook_attn_in): HookPoint()
      (hook_q_input): HookPoint()
      (hook_k_input): HookPoint()
      (hook_v_input): HookPoint()
      (hook_mlp_in): HookPoint()
      (hook_attn_out): HookPoint()
      (hook_mlp_out): HookPoint()
      (hook_resid_pre): HookPoint()
      (hook_resid_mid): HookPoint()
      (hook_resid_post): HookPoint()
    )
  )
  (ln_final): LayerNormPre(
    (hook_scale): HookPoint()
    (hook_normalized): HookPoint()
  )
  (unembed): Unembed()
)
Kept verb forms for language: cat

after filter_conjugations, number of saved verbs: 1562
[catalan] verbs kept: 500
[catalan] after prepare_language_dataset  first_prompts=500  second_prompts=500
after accuracy_filter, number of sentences saved: 72 / 500 72 / 500
✅ Saved: resid_pre_first2second_catalan_inf3_conj4.png
✅ Saved: resid_pre_first2second_catalan_inf3_conj3.png
✅ Saved: resid_pre_first2second_catalan_inf2_conj3.png
✅ Saved: resid_pre_first2second_catalan_inf2_conj2.png
✅ Saved: resid_pre_first2second_catalan_inf1_conj3.png
✅ Saved: resid_pre_second2first_catalan_inf3_conj4.png
✅ Saved: resid_pre_second2first_catalan_inf2_conj3.png
✅ Saved: resid_pre_second2first_catalan_inf3_conj3.png
✅ Saved: resid_pre_second2first_catalan_inf2_conj2.png
✅ Saved: attn_head_out_all_pos_second2first_catalan.png
✅ Saved tensor: attn_head_out_all_pos_patch_results_second2first_catalan.pt
✅ Saved: attn_head_out_all_pos_first2second_catalan.png
✅ Saved tensor: attn_head_out_all_pos_patch_results_first2second_catalan.pt

=== Running activation patching for czech (ces) ===
Loaded pretrained model bigscience/bloom-1b1 into HookedTransformer
model: HookedTransformer(
  (embed): Embed(
    (ln): LayerNorm(
      (hook_scale): HookPoint()
      (hook_normalized): HookPoint()
    )
  )
  (hook_embed): HookPoint()
  (pos_embed): PosEmbed()
  (hook_pos_embed): HookPoint()
  (blocks): ModuleList(
    (0-23): 24 x TransformerBlock(
      (ln1): LayerNormPre(
        (hook_scale): HookPoint()
        (hook_normalized): HookPoint()
      )
      (ln2): LayerNormPre(
        (hook_scale): HookPoint()
        (hook_normalized): HookPoint()
      )
      (attn): Attention(
        (hook_k): HookPoint()
        (hook_q): HookPoint()
        (hook_v): HookPoint()
        (hook_z): HookPoint()
        (hook_attn_scores): HookPoint()
        (hook_pattern): HookPoint()
        (hook_result): HookPoint()
      )
      (mlp): MLP(
        (hook_pre): HookPoint()
        (hook_post): HookPoint()
      )
      (hook_attn_in): HookPoint()
      (hook_q_input): HookPoint()
      (hook_k_input): HookPoint()
      (hook_v_input): HookPoint()
      (hook_mlp_in): HookPoint()
      (hook_attn_out): HookPoint()
      (hook_mlp_out): HookPoint()
      (hook_resid_pre): HookPoint()
      (hook_resid_mid): HookPoint()
      (hook_resid_post): HookPoint()
    )
  )
  (ln_final): LayerNormPre(
    (hook_scale): HookPoint()
    (hook_normalized): HookPoint()
  )
  (unembed): Unembed()
)
Kept verb forms for language: ces

after filter_conjugations, number of saved verbs: 326
[czech] verbs kept: 326
[czech] afteafter accuracy_filter, number of sentences saved: 19 / 500 0 / 500
↪️  first2second: skipping (4, 4) - no match
↪️  first2second: skipping (3, 3) - no match
↪️  first2second: skipping (3, 4) - no match
↪️  Skipping second2first - zero aligned prompts
↪️  Skipping first2second - zero aligned prompts

=== Running activation patching for english (eng) ===
Loaded pretrained model bigscience/bloom-1b1 into HookedTransformer
model: HookedTransformer(
  (embed): Embed(
    (ln): LayerNorm(
      (hook_scale): HookPoint()
      (hook_normalized): HookPoint()
    )
  )
  (hook_embed): HookPoint()
  (pos_embed): PosEmbed()
  (hook_pos_embed): HookPoint()
  (blocks): ModuleList(
    (0-23): 24 x TransformerBlock(
      (ln1): LayerNormPre(
        (hook_scale): HookPoint()
        (hook_normalized): HookPoint()
      )
      (ln2): LayerNormPre(
        (hook_scale): HookPoint()
        (hook_normalized): HookPoint()
      )
      (attn): Attention(
        (hook_k): HookPoint()
        (hook_q): HookPoint()
        (hook_v): HookPoint()
        (hook_z): HookPoint()
        (hook_attn_scores): HookPoint()
        (hook_pattern): HookPoint()
        (hook_result): HookPoint()
      )
      (mlp): MLP(
        (hook_pre): HookPoint()
        (hook_post): HookPoint()
      )
      (hook_attn_in): HookPoint()
      (hook_q_input): HookPoint()
      (hook_k_input): HookPoint()
      (hook_v_input): HookPoint()
      (hook_mlp_in): HookPoint()
      (hook_attn_out): HookPoint()
      (hook_mlp_out): HookPoint()
      (hook_resid_pre): HookPoint()
      (hook_resid_mid): HookPoint()
      (hook_resid_post): HookPoint()
    )
  )
  (ln_final): LayerNormPre(
    (hook_scale): HookPoint()
    (hook_normalized): HookPoint()
  )
  (unembed): Unembed()
)
Kept verb forms for language: eng

after filter_conjugations, number of saved verbs: 23896
[english] verbs kept: 500
[english] after prepare_language_dataset  first_prompts=500  second_prompts=500
after accuracy_filter, number of sentences saved: 121 / 500 119 / 500
✅ Saved: resid_pre_first2second_english_inf3_conj3.png
✅ Saved: resid_pre_first2second_english_inf4_conj4.png
✅ Saved: resid_pre_first2second_english_inf6_conj6.png
✅ Saved: resid_pre_first2second_english_inf5_conj5.png
✅ Saved: resid_pre_second2first_english_inf3_conj3.png
✅ Saved: resid_pre_second2first_english_inf4_conj4.png
✅ Saved: resid_pre_second2first_english_inf6_conj6.png
✅ Saved: resid_pre_second2first_english_inf5_conj5.png
✅ Saved: attn_head_out_all_pos_second2first_english.png
✅ Saved tensor: attn_head_out_all_pos_patch_results_second2first_english.pt
Loaded pretrained model bigscience/bloom-1b1 into HookedTransformer
model: HookedTransformer(
  (embed): Embed(
    (ln): LayerNorm(
      (hook_scale): HookPoint()
      (hook_normalized): HookPoint()
    )
  )
  (hook_embed): HookPoint()
  (pos_embed): PosEmbed()
  (hook_pos_embed): HookPoint()
  (blocks): ModuleList(
    (0-23): 24 x TransformerBlock(
      (ln1): LayerNormPre(
        (hook_scale): HookPoint()
        (hook_normalized): HookPoint()
      )
      (ln2): LayerNormPre(
        (hook_scale): HookPoint()
        (hook_normalized): HookPoint()
      )
      (attn): Attention(
        (hook_k): HookPoint()
        (hook_q): HookPoint()
        (hook_v): HookPoint()
        (hook_z): HookPoint()
        (hook_attn_scores): HookPoint()
        (hook_pattern): HookPoint()
        (hook_result): HookPoint()
      )
      (mlp): MLP(
        (hook_pre): HookPoint()
        (hook_post): HookPoint()
      )
      (hook_attn_in): HookPoint()
      (hook_q_input): HookPoint()
      (hook_k_input): HookPoint()
      (hook_v_input): HookPoint()
      (hook_mlp_in): HookPoint()
      (hook_attn_out): HookPoint()
      (hook_mlp_out): HookPoint()
      (hook_resid_pre): HookPoint()
      (hook_resid_mid): HookPoint()
      (hook_resid_post): HookPoint()
    )
  )
  (ln_final): LayerNormPre(
    (hook_scale): HookPoint()
    (hook_normalized): HookPoint()
  )
  (unembed): Unembed()
)
Kept verb forms for language: fin

after filter_✅ Saved: resid_pre_first2second_french_inf3_conj3.png
ept: 0
[finnish] after prepare_language_dataset  first_prompts=0  second_prompts=0
after accuracy_filter, number of sentences saved: 0 / 0 0 / 0
↪️  Skipping second2first - zero aligned prompts
↪️  Skipping first2second - zero aligned prompts

=== Running activation patching for french (fra) ===
Loaded pretrained model bigscience/bloom-1b1 into HookedTransformer
model: HookedTransformer(
  (embed): Embed(
    (ln): LayerNorm(
      (hook_scale): HookPoint()
      (hook_normalized): HookPoint()
    )
  )
  (hook_embed): HookPoint()
  (pos_embed): PosEmbed()
  (hook_pos_embed): HookPoint()
  (blocks): ModuleList(
    (0-23): 24 x TransformerBlock(
      (ln1): LayerNormPre(
        (hook_scale): HookPoint()
        (hook_normalized): HookPoint()
      )
      (ln2): LayerNormPre(
        (hook_scale): HookPoint()
        (hook_normalized): HookPoint()
      )
      (attn): Attention(
        (hook_k): HookPoint()
        (hook_q): HookPoint()
        (hook_v): HookPoint()
        (hook_z): HookPoint()
        (hook_attn_scores): HookPoint()
        (hook_pattern): HookPoint()
        (hook_result): HookPoint()
      )
      (mlp): MLP(
        (hook_pre): HookPoint()
        (hook_post): HookPoint()
      )
      (hook_attn_in): HookPoint()
      (hook_q_input): HookPoint()
      (hook_k_input): HookPoint()
      (hook_v_input): HookPoint()
      (hook_mlp_in): HookPoint()
      (hook_attn_out): HookPoint()
      (hook_mlp_out): HookPoint()
      (hook_resid_pre): HookPoint()
      (hook_resid_mid): HookPoint()
      (hook_resid_post): HookPoint()
    )
  )
  (ln_final): LayerNormPre(
    (hook_scale): HookPoint()
    (hook_normalized): HookPoint()
  )
  (unembed): Unembed()
)
Kept verb forms for language: fra

after filter_conjugations, number of saved verbs: 4754
[french] verbs kept: 500
[french] after prepare_language_dataset  first_prompts=500  second_prompts=500
after accuracy_filter, number of sentences saved: 74 / 500 68 / 500
✅ Saved: resid_pre_first2second_french_inf3_conj4.png
✅ Saved: resid_pre_second2first_french_inf3_conj3.png
ed: resid_pre_first2second_french_inf2_conj3.png
✅ Saved: resid_pre_second2first_french_inf4_conj4.png
✅ Saved: resid_pre_second2first_french_inf2_conj3.png
✅ Saved: resid_pre_second2first_french_inf3_conj4.png
✅ Saved: resid_pre_second2first_french_inf5_conj5.png
✅ Saved: attn_head_out_all_pos_second2first_french.png
✅ Saved tensor: attn_head_out_all_pos_patch_results_second2first_french.pt
✅ Saved: attn_head_out_all_pos_first2second_french.png
✅ Saved tensor: attn_head_out_all_pos_patch_results_first2second_french.pt

=== Running activation patching for serbo-croatian (hbs) ===
Loaded pretrained model bigscience/bloom-1b1 into HookedTransformer
model: HookedTransformer(
  (embed): Embed(
    (ln): LayerNorm(
      (hook_scale): HookPoint()
      (hook_normalized): HookPoint()
    )
  )
  (hook_embed): HookPoint()
  (pos_embed): PosEmbed()
  (hook_pos_embed): HookPoint()
  (blocks): ModuleList(
    (0-23): 24 x TransformerBlock(
      (ln1): LayerNormPre(
        (hook_scale): HookPoint()
        (hook_normalized): HookPoint()
      )
      (ln2): LayerNormPre(
        (hook_scale): HookPoint()
        (hook_normalized): HookPoint()
      )
      (attn): Attention(
        (hook_k): HookPoint()
        (hook_q): HookPoint()
        (hook_v): HookPoint()
        (hook_z): HookPoint()
        (hook_attn_scores): HookPoint()
        (hook_pattern): HookPoint()
        (hook_result): HookPoint()
      )
      (mlp): MLP(
        (hook_pre): HookPoint()
        (hook_post): HookPoint()
      )
      (hook_attn_in): HookPoint()
      (hook_q_input): HookPoint()
      (hook_k_input): HookPoint()
      (hook_v_input): HookPoint()
      (hook_mlp_in): HookPoint()
      (hook_attn_out): HookPoint()
      (hook_mlp_out): HookPoint()
      (hook_resid_pre): HookPoint()
      (hook_resid_mid): HookPoint()
      (hook_resid_post): HookPoint()
    )
  )
  (ln_final): LayerNormPre(
    (hook_scale): HookPoint()
    (hooafter accuracy_filter, number of sentences saved: 0 / 0 0 / 0
↪️  Skipping second2first - zero aligned prompts
↪️  Skipping first2second - zero aligned prompts

=== Running activation patching for hungarian (hun) ===
Loaded pretr✅ Saved: resid_pre_first2second_hungarian_inf4_conj5.png
↪️  first2second: skipping (5, 6) - no match
↪️  first2second: skipping (3, 4) - no match
  (hook_normalized): HookPoint()
    )
  )
  (hook_embed): HookPoint()
  (pos_embed): PosEmbed()
  (hook_pos_embed): HookPoint()
  (blocks): ModuleList(
    (0-23): 24 x TransformerBlock(
      (ln1): LayerNormPre(
        (hook_scale): HookPoint()
        (hook_normalized): HookPoint()
      )
      (ln2): LayerNormPre(
        (hook_scale): HookPoint()
        (hook_normalized): HookPoint()
      )
      (attn): Attention(
        (hook_k): HookPoint()
        (hook_q): HookPoint()
        (hook_v): HookPoint()
        (hook_z): HookPoint()
        (hook_attn_scores): HookPoint()
        (hook_pattern): HookPoint()
        (hook_result): HookPoint()
      )
      (mlp): MLP(
        (hook_pre): HookPoint()
        (hook_post): HookPoint()
      )
      (hook_attn_in): HookPoint()
      (hook_q_input): HookPoint()
      (hook_k_input): HookPoint()
      (hook_v_input): HookPoint()
      (hook_mlp_in): HookPoint()
      (hook_attn_out): HookPoint()
      (hook_mlp_out): HookPoint()
      (hook_resid_pre): HookPoint()
      (hook_resid_mid): HookPoint()
      (hook_resid_post): HookPoint()
    )
  )
  (ln_final): LayerNormPre(
    (hook_scale): HookPoint()
    (hook_normalized): HookPoint()
  )
  (unembed): Unembed()
)
Kept verb forms for language: hun

after filter_conjugations, number of saved verbs: 657
[hungarian] verbs kept: 500
[hungarian] after prepare_language_dataset  first_prompts=500  second_prompts=500
after accuracy_filter, number of sentences saved: 3 / 500 1 / 500
✅ Saved: attn_head_out_all_pos_first2second_hungarian.png
✅ Saved tensor: attn_head_out_all_pos_patch_results_first2second_hungarian.pt

=== Running activation patching for italian (ita) ===
Load✅ Saved: resid_pre_first2second_italian_inf4_conj4.png
former
model: HookedTransformer(
  (embed): Embed(
    (ln): LayerNorm(
      (hook_scale): HookPoint()
      (hook_normalized): HookPoint()
    )
  )
  (hook_embed): HookPoint()
  (pos_embed): PosEmbed()
  (hook_pos_embed): HookPoint()
  (blocks): ModuleList(
    (0-23): 24 x TransformerBlock(
      (ln1): LayerNormPre(
        (hook_scale): HookPoint()
        (hook_normalized): HookPoint()
      )
      (ln2): LayerNormPre(
        (hook_scale): HookPoint()
        (hook_normalized): HookPoint()
      )
      (attn): Attention(
        (hook_k): HookPoint()
        (hook_q): HookPoint()
        (hook_v): HookPoint()
        (hook_z): HookPoint()
        (hook_attn_scores): HookPoint()
        (hook_pattern): HookPoint()
        (hook_result): HookPoint()
      )
      (mlp): MLP(
        (hook_pre): HookPoint()
        (hook_post): HookPoint()
      )
      (hook_attn_in): HookPoint()
      (hook_q_input): HookPoint()
      (hook_k_input): HookPoint()
      (hook_v_input): HookPoint()
      (hook_mlp_in): HookPoint()
      (hook_attn_out): HookPoint()
      (hook_mlp_out): HookPoint()
      (hook_resid_pre): HookPoint()
      (hook_resid_mid): HookPoint()
      (hook_resid_post): HookPoint()
    )
  )
  (ln_final): LayerNormPre(
    (hook_scale): HookPoint()
    (hook_normalized): HookPoint()
  )
  (unembed): Unembed()
)
Kept verb forms for language: ita

after filter_conjugations, number of saved verbs: 6776
[italian] verbs kept: 500
[italian] after prepare_language_dataset  first_prompts=500  second_prompts=500
after accuracy_filter, number of sentences saved: 38 / 500 40 / 500
✅ Saved: resid_pre_first2second_italian_inf5_conj5.png
✅ Saved: resid_pre_first2second_italian_inf4_conj3.png
✅ Saved: resid_pre_second2first_italian_inf4_conj3.png
✅ Saved: resid_pre_second2first_italian_inf4_conj4.png
✅ Saved: resid_pre_second2first_italian_inf6_conj5.png
✅ Saved: resid_pre_second2first_italian_inf5_conj5.png
✅ Saved: attn_head_out_all_pos_second2first_italian.png
✅ Saved tensor: attn_head_out_all_pos_patch_results_second2first_italian.pt
after accuracy_filter, number of sentences saved: 0 / 0 0 / 0
↪️  Skipping second2first - zero aligned prompts
↪️  Skipping first2second - zero aligned prompts

=== Running activation patching for polish (pol) ===
Loaded pretrained model bigscience/bloom-1b1 into HookedTransformer
model: HookedTransformer(
  (embed): Embed(
    (ln): LayerNorm(
      (hook_scale): HookPoint()
      (hook_normalized): HookPoint()
    )
  )
  (hook_embed): HookPoint()
  (pos_embed): PosEmbed()
  (hook_pos_embed): HookPoint()
  (blocks): ModuleList(
    (0-23): 24 x TransformerBlock(
      (ln1): LayerNormPre(
        (hook_scale): HookPoint()
        (hook_normalized): HookPoint()
      )
      (ln2): LayerNormPre(
        (hook_scale): HookPoint()
        (hook_normalized): HookPoint()
      )
      (attn): Attention(
        (hook_k): HookPoint()
        (hook_q): HookPoint()
        (hook_v): HookPoint()
        (hook_z): HookPoint()
        (hook_attn_scores): HookPoint()
        (hook_pattern): HookPoint()
        (hook_result): HookPoint()
      )
      (mlp): MLP(
        (hook_pre): HookPoint()
        (hook_post): HookPoint()
      )
      (hook_attn_in): HookPoint()
      (hook_q_input): HookPoint()
      (hook_k_input): HookPoint()
      (hook_v_input): HookPoint()
      (hook_mlp_in): HookPoint()
      (hook_attn_out): HookPoint()
      (hook_mlp_out): HookPoint()
      (hook_resid_pre): HookPoint()
      (hook_resid_mid): HookPoint()
      (hook_resid_post): HookPoint()
    )
  )
  (ln_final): LayerNormPre(
    (hook_scale): HookPoint()
    (hook_normalized): HookPoint()
  )
  (unembed): Unembed()
)
Kept verb forms for language: pol

No data found for language: pol
[polish] verbs kept: 0
[polish] after prepare_language_dataset  first_prompts=0  second_promptsafter accuracy_filter, number of sentences saved: 46 / 500 74 / 500
✅ Saved: resid_pre_first2second_portuguese_inf3_conj3.png
ipping first2second - zero aligned prompts

=== Running activation patching for portuguese (por) ===
Loaded pretrained model bigscience/bloom-1b1 into HookedTransformer
model: HookedTransformer(
  (embed): Embed(
    (ln): LayerNorm(
      (hook_scale): HookPoint()
      (hook_normalized): HookPoint()
    )
  )
  (hook_embed): HookPoint()
  (pos_embed): PosEmbed()
  (hook_pos_embed): HookPoint()
  (blocks): ModuleList(
    (0-23): 24 x TransformerBlock(
      (ln1): LayerNormPre(
        (hook_scale): HookPoint()
        (hook_normalized): HookPoint()
      )
      (ln2): LayerNormPre(
        (hook_scale): HookPoint()
        (hook_normalized): HookPoint()
      )
      (attn): Attention(
        (hook_k): HookPoint()
        (hook_q): HookPoint()
        (hook_v): HookPoint()
        (hook_z): HookPoint()
        (hook_attn_scores): HookPoint()
        (hook_pattern): HookPoint()
        (hook_result): HookPoint()
      )
      (mlp): MLP(
        (hook_pre): HookPoint()
        (hook_post): HookPoint()
      )
      (hook_attn_in): HookPoint()
      (hook_q_input): HookPoint()
      (hook_k_input): HookPoint()
      (hook_v_input): HookPoint()
      (hook_mlp_in): HookPoint()
      (hook_attn_out): HookPoint()
      (hook_mlp_out): HookPoint()
      (hook_resid_pre): HookPoint()
      (hook_resid_mid): HookPoint()
      (hook_resid_post): HookPoint()
    )
  )
  (ln_final): LayerNormPre(
    (hook_scale): HookPoint()
    (hook_normalized): HookPoint()
  )
  (unembed): Unembed()
)
Kept verb forms for language: por

after filter_conjugations, number of saved verbs: 2844
[portuguese] verbs kept: 500
[portuguese] after prepare_language_dataset  first_prompts=500  second_prompts=500
✅ Saved: resid_pre_second2first_portuguese_inf3_conj3.png
✅ Saved: resid_pre_second2first_portuguese_inf2_conj2.png
✅ Saved: attn_head_out_all_pos_second2first_portuguese.png
✅ Saved tensor: attn_head_out_all_pos_patch_results_second2first_portuguese.pt
, 4) - no match
✅ Saved: attn_head_out_all_pos_first2second_portuguese.png
✅ Saved tensor: attn_head_out_all_pos_patch_results_first2second_portuguese.pt

=== Running activation patching for russian (rus) ===
Loaded pretrained model bigscience/bloom-1b1 into HookedTransformer
model: HookedTransformer(
  (embed): Embed(
    (ln): LayerNorm(
      (hook_scale): HookPoint()
      (hook_normalized): HookPoint()
    )
  )
  (hook_embed): HookPoint()
  (pos_embed): PosEmbed()
  (hook_pos_embed): HookPoint()
  (blocks): ModuleList(
    (0-23): 24 x TransformerBlock(
      (ln1): LayerNormPre(
        (hook_scale): HookPoint()
        (hook_normalized): HookPoint()
      )
      (ln2): LayerNormPre(
        (hook_scale): HookPoint()
        (hook_normalized): HookPoint()
      )
      (attn): Attention(
        (hook_k): HookPoint()
        (hook_q): HookPoint()
        (hook_v): HookPoint()
        (hook_z): HookPoint()
        (hook_attn_scores): HookPoint()
        (hook_pattern): HookPoint()
        (hook_result): HookPoint()
      )
      (mlp): MLP(
        (hook_pre): HookPoint()
        (hook_post): HookPoint()
      )
      (hook_attn_in): HookPoint()
      (hook_q_input): HookPoint()
      (hook_k_input): HookPoint()
      (hook_v_input): HookPoint()
      (hook_mlp_in): HookPoint()
      (hook_attn_out): HookPoint()
      (hook_mlp_out): HookPoint()
      (hook_resid_pre): HookPoint()
      (hook_resid_mid): HookPoint()
      (hook_resid_post): HookPoint()
    )
  )
  (ln_final): LayerNormPre(
    (hook_scale): HookPoint()
    (hook_normalized): HookPoint()
  )
  (unembed): Unembed()
)
Kept verb forms for language: rus

after filter_conjugations, number of saved verbs: 3509
[russian] verbs kept: 500
[russian] after prepare_language_dataset  first_prompts=500  second_prompts=500
after accuracy_filter, number of sentences saved: 102 / 500 48 / 500
✅ Saved: resid_pre_first2second_russian_inf5_conj6.png
✅ Saved: resid_pre_first2second_russian_inf4_conj5.png
✅ Saved: resid_pre_first2second_russian_inf5_conj5.png
✅ Saved: resid_pre_second2first_russian_inf5_conj6.png
✅ Saved: resid_pre_second2first_russian_inf4_conj5.png
✅ Saved: resid_pre_second2first_russian_inf5_conj5.png
✅ Saved: attn_head_out_all_pos_second2first_russian.png
✅ Saved tensor: attn_head_out_all_pos_patch_results_second2first_russian.pt
✅ Saved: attn_head_out_all_pos_first2second_russian.png
✅ Saved tensor: attn_head_out_all_pos_patch_results_first2second_russian.pt

=== Running activation patching for spanish (spa) ===
Loaded pretrained model bigscience/bloom-1b1 into HookedTransformer
model: HookedTransformer(
  (embed): Embed(
    (ln): LayerNorm(
      (hook_scale): HookPoint()
      (hook_normalized): HookPoint()
    )
  )
  (hook_embed): HookPoint()
  (pos_embed): PosEmbed()
  (hook_pos_embed): HookPoint()
  (blocks): ModuleList(
    (0-23): 24 x TransformerBlock(
      (ln1): LayerNormPre(
        (hook_scale): HookPoint()
        (hook_normalized): HookPoint()
      )
      (ln2): LayerNormPre(
        (hook_scale): HookPoint()
        (hook_normalized): HookPoint()
      )
      (attn): Attention(
        (hook_k): HookPoint()
        (hook_q): HookPoint()
        (hook_v): HookPoint()
        (hook_z): HookPoint()
        (hook_attn_scores): HookPoint()
        (hook_pattern): HookPoint()
        (hook_result): HookPoint()
      )
      (mlp): MLP(
        (hook_pre): HookPoint()
        (hook_post): HookPoint()
      )
      (hook_attn_in): HookPoint()
      (hook_q_input): HookPoint()
      (hook_k_input): HookPoint()
      (hook_v_input): HookPoint()
      (hook_mlp_in): HookPoint()
      (hook_attn_out): HookPoint()
      (hook_mlp_out): HookPoint()
      (hook_resid_pre): HookPoint()
      (hook_resid_mid): HookPoint()
      (hook_resid_post): HookPoint()
    )
  )
  (ln_final): LayerNormPre(
    (hook_scale): HookPoint()
    (hook_normalized): HookPoint()
  )
  (unembed): Unembed()
)
Kept verb forms for language: spa

after filter_conjugations, number of saved verbs: 4737
[spanish] verbs kept: 500
[spanish] after prepare_language_dataset  first_prompts=500  second_prompts=500✅ Saved: resid_pre_first2second_spanish_inf3_conj3.png
00 99 / 500
✅ Saved: resid_pre_first2second_spanish_inf2_conj3.png
✅ Saved: resid_pre_first2second_spanish_inf4_conj5.png
✅ Saved: resid_pre_second2first_spanish_inf4_conj4.png
✅ Saved: resid_pre_second2first_spanish_inf2_conj3.png
✅ Saved: resid_pre_second2first_spanish_inf4_conj5.png
✅ Saved: attn_head_out_all_pos_second2first_spanish.png
✅ Saved tensor: attn_head_out_all_pos_patch_results_second2first_spanish.pt
after accuracy_filter, number of sentences saved: 0 / 0 0 / 0
↪️  Skipping second2first - zero aligned prompts
↪️  Skipping first2second - zero aligned prompts
 for swedish (swe) ===
Loaded pretrained model bigscience/bloom-1b1 into HookedTransformer
model: HookedTransformer(
  (embed): Embed(
    (ln): LayerNorm(
      (hook_scale): HookPoint()
      (hook_normalized): HookPoint()
    )
  )
  (hook_embed): HookPoint()
  (pos_embed): PosEmbed()
  (hook_pos_embed): HookPoint()
  (blocks): ModuleList(
    (0-23): 24 x TransformerBlock(
      (ln1): LayerNormPre(
        (hook_scale): HookPoint()
        (hook_normalized): HookPoint()
      )
      (ln2): LayerNormPre(
        (hook_scale): HookPoint()
        (hook_normalized): HookPoint()
      )
      (attn): Attention(
        (hook_k): HookPoint()
        (hook_q): HookPoint()
        (hook_v): HookPoint()
        (hook_z): HookPoint()
        (hook_attn_scores): HookPoint()
        (hook_pattern): HookPoint()
        (hook_result): HookPoint()
      )
      (mlp): MLP(
        (hook_pre): HookPoint()
        (hook_post): HookPoint()
      )
      (hook_attn_in): HookPoint()
      (hook_q_input): HookPoint()
      (hook_k_input): HookPoint()
      (hook_v_input): HookPoint()
      (hook_mlp_in): HookPoint()
      (hook_attn_out): HookPoint()
      (hook_mlp_out): HookPoint()
      (hook_resid_pre): HookPoint()
      (hook_resid_mid): HookPoint()
      (hook_resid_post): HookPoint()
    )
  )
  (ln_final): LayerNormPre(
    (hook_scale): HookPoint()
    (hook_normalized): HookPoint()
  )
  (unembed): Unembed()
)
Kept verb forms for language: swe

after filter_conjugations, number of saved verbs: 0
[swedish] verbs kept: 0
[swedish] after prepare_language_dataset  first_prompts=0  second_prompts=0
